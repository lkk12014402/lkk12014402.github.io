---
layout:     post
title:      "树莓派无人驾驶小汽车总结"
subtitle:   "Keras"
date:       2018-01-14
author:     "hadxu"
header-img: "img/in-post/IMG_0615.JPG"
tags:
    - Tensorflow
    - Python
    - Keras
---

这学期开学买了一个树莓派以及小车套装，玩起了树莓派的无人驾驶小汽车，从头搭建一个树莓派小车，从头到尾还是学到了很多很多东西。下面我来总结一下整个的学习过程。其实搭建树莓派小车的过程与我们一般的图像识别区别不大，因此学过图像识别的，搭建树莓派小车也就很简单了，但是说起来简单，里面有各种各样的问题待我们来解决。

1. 安装树莓派，搭建树莓派环境

**这部分内容是非常重要的，因为直接影响我们后期的小车的完成，树莓派的搭建如下**

    1. 简称`raspberry`
    2. 当我们拿到树莓派第一件事就是烧系统，使用`etcher`
    3. 在sd卡的主文件夹下新建ssh文件，因为新版系统默认不支持ssh
    4. 得到ip地址 通过 `arp -a`或者`ping raspberrypi.local`
    5. 连接树莓派通过 `ssh pi@ip` 密码 `raspberry`
    6. 进行配置`sudo raspi-config`
        
        * 设置camera为Yes，VNC为YES
        * 通过vnc连接 软件为`vnc viewer`

    7. 安装深度学习框架TensorFlow以及keras。版本分别为1.1.0,2.1.2
    > 因为需要版本需要在机器上以及树莓派上都要一致，避免出现不必要的麻烦。

```
sudo pip3 install –v tensorflow-1.1.0-cp35-cp35m-linux_armv71.whl

2. 安装Keras:

    - sudo pip3 install –v numpy

    - sudo apt-get install python3-scipy

    - sudo pip3 install –v scikit-learn

    - sudo pip3 install –v pillow

    - sudo apt-get install python3-h5py

    - sudo pip3 install –v keras
```

2. 完成车身制作。

主要在于树莓派与电机驱动板之间的连接

> 首先是驱动板与电机之间的连接，驱动板型号为L298N,上面有两个驱动，因为不仅仅驱动电机，还要驱动前轮，将电机与前轮与驱动板连接起来，同时将树莓派与驱动板连接起来，一共是7个孔，分别为前后轮4个，两个信号线以及一个地线。

![](/img/IMG_0615.JPG)

![](/img/IMG_0616.JPG)
![](/img/IMG_0617.JPG)
![](/img/IMG_0618.JPG)


3. 学会使用picamera。

**在picamera中，我遇到了一个非常大的麻烦，就是有段时间摄像头没有用，我以为是树莓派坏了，以为软件坏了，后来查了很多很多资料，原来是摄像头松动了。。。**

4. 使用电脑来控制树莓派采集数据。

**这部分是热身实验，该树莓派小车采集数据的方法为使用w,a,s,d控制树莓派小车的上下左右，同时将键盘的信息以及摄像头拍到的信息同时存为一张文件，然后供训练读取。文件名即为监督学习的label，图像为训练的特征。**

5. 训练采集到的数据

**这时候数据是在树莓派中的，如何将其运到我们的服务器上呢**

```
sudo python3 -m http.server
```

> 一旦我们将数据弄到服务器上，那就特别简单了，首先将label进行one-hot操作，然后进行卷积神经网络训练,卷积神经网络代码如下，keras代码特别简单。

```
def build_model(keep_prob):
    """
    NVIDIA model used
    Image normalization to avoid saturation and make gradients work better.
    Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU
    Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU
    Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU
    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU
    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU
    Drop out (0.5)
    Fully connected: neurons: 100, activation: ELU
    Fully connected: neurons: 50, activation: ELU
    Fully connected: neurons: 10, activation: ELU
    Fully connected: neurons: 1 (output)

    # the convolution layers are meant to handle feature engineering
    the fully connected layer for predicting the steering angle.
    dropout avoids overfitting
    ELU(Exponential linear unit) function takes care of the Vanishing gradient problem. 
    """
    # IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 240, 240, 3
    # INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)
    
    model = Sequential()
    model.add(Lambda(lambda x: (x/102.83-1), input_shape=INPUT_SHAPE))#/85-5139
    model.add(Conv2D(24, (5, 5), activation='elu', strides=(2, 2)))
    model.add(Conv2D(36, (5, 5), activation='elu', strides=(2, 2)))
    model.add(Conv2D(48, (5, 5), activation='elu', strides=(2, 2)))

    #model.add(Dropout(0.5))
    model.add(Conv2D(64, (3, 3), activation='elu'))
    #model.add(Dropout(0.3))
    model.add(Conv2D(64, (3, 3), activation='elu'))
    model.add(Dropout(keep_prob))
    model.add(Flatten())
    model.add(Dense(500, activation='elu'))
    #model.add(Dropout(0.1))
    model.add(Dense(250, activation='elu'))
    #model.add(Dropout(0.1))
    model.add(Dense(50, activation='elu'))
    #model.add(Dropout(0.1))
    model.add(Dense(5))
    model.summary()

    return model
```

6. 进行预测

```
predictions_array = model.predict(camera_data_array, batch_size=20, verbose=1)
```
注意，这里得到的为每个操作的概率，需要进行最大化操作，即能提取最大化的操作数，从而实现预测。

该项目代码在github上[auto-driver](https://github.com/HadXu/auto-drive)

