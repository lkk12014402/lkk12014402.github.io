---
layout:     post
title:      "TVM-End2End DL framework"
subtitle:   "CUDA C++ Python"
date:       2018-05-03
author:     "hadxu"
header-img: "img/in-post/Thunder/thunder.jpg"
tags:
    - Python
    - TVM
---

华盛顿大学的陈天奇提出的```TVM-end2end```框架是跨平台的一个深度学习框架，看看整体的架构

![](/img/in-post/Thunder/TVM.jpg)

1. 最上层是我们常用的各种深度学习框架，比如```tf、Pytorch、MxNET```等等。
2. 中间层是```IR```,一种中间表示的结构，常用的为```llvm```语言。
3. 最底层为各个硬件的支持，通过```IR```来生成对应架构的代码，实现重用。

### 安装

> 在自己的Linux以及MAC测试安装成功，win没有。

1. 首先clone

```bash
git clone --recursive https://github.com/dmlc/tvm
```

2. 下载```llvm >= 4.0```,[llvm](http://releases.llvm.org/download.html),并解压到自定义的文件夹，更改```/make/config.mk```文件，llvm设置为刚刚解压的路径
并设置各种参数

```bash
USE_CUDA = 1
CUDA_PATH = /usr/local/cuda
LLVM_CONFIG=/path/to/your/llvm/bin/llvm-config
```

3. 编译 ```make -j8```

到此，TVM编译成功，我们需要将其注册到Python环境中

```bash
cd python; python setup.py install --user; cd ..
cd topi/python; python setup.py install --user; cd ../..
```


### 使用

首先看看简单的加法如何做？


1. 导入包

```python
import tvm
import numpy as np
```

2. 定义各种参数

```python
# 定义host
tgt_host="llvm"

# 定义设备
tgt="llvm"

# 定义类型
dtype = "float32"

# 定义上下文
ctx = tvm.context(tgt, 0)
```

3. 定义数据

```python
shape = (1, 5)
x = np.random.uniform(0, 10, size=shape).astype(dtype)
y = np.random.uniform(0, 10, size=shape).astype(dtype)
z = np.zeros(shape).astype(dtype)

# 将数据移动到设备上 这里使用llvm，也就是CPU
arr_x = tvm.nd.array(x, ctx=ctx)
arr_y = tvm.nd.array(y, ctx=ctx)
arr_z = tvm.nd.array(z, ctx=ctx)
```

3. 定义加法操作

```python
A = tvm.placeholder(shape, dtype=dtype, name="A")
B = tvm.placeholder(shape, dtype=dtype, name="B")
C = tvm.compute(A.shape, lambda *i: A(*i) + B(*i))

# 将表达式转换为代码
s = tvm.create_schedule(C.op)
```

4. 计算

```python
fadd = tvm.build(s, [A, B, C], tgt, target_host=tgt_host, name='add')

fadd(arr_x,arr_y,arr_z)

res = arr_z.asnumpy()
```

5. 测试

```python
np.testing.assert_allclose(x + y, res, rtol=1e-5)
```


### 按照官方的说法，将```llvm```换成```cuda```即可完成GPU端的计算，但是我没有成功，报错需要进行```bind```?如果```bind```需要的话，那么我觉得TVM并没有什么特别之处？也许我的错误吧？


> 不过总体来说，TVM提供了一种非常nice的思想，使用中间表达来生成对应硬件的机器码，这是非常有用的？





