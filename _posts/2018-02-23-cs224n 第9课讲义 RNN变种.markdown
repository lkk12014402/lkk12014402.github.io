---
layout:     post
title:      "CS224N 第9课"
subtitle:   "cs224n"
date:       2018-02-23
author:     "hadxu"
header-img: "img/in-post/cs224n/cs224n_head.png"
tags:
    - Tensorflow
    - Python
    - cs224n
---

# RNN变种

## 梯度消失

> 梯度弥散很好解决，直接使用梯度切断即可。而梯度消失却很难解决。于是提出了各种各样的变种RNN。

### GRU

![](/img/in-post/cs224n/fig22.jpg)

### LSTM （[LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)）
 
![](/img/in-post/cs224n/fig23.jpg)

![](/img/in-post/cs224n/fig24.jpg)

### 双向RNN

![](/img/in-post/cs224n/fig25.jpg)


### GRU与LSTM的对比

![](/img/in-post/cs224n/fig26.jpg)

# 第10课 高级NLP内容

* 机器翻译
* Seq2Seq
* 注意力机制

## 机器翻译(Machine Translation (MT))

> 机器翻译是一种非常重要的技术，将一种语言翻译成另一种语言


### 起源 

机器翻译起源于1950年，大部分都是俄语与英语的翻译(主要是冷战。。。)，那个时候非翻译都是基于规则的翻译方式，使用词与词的映射关系。一个非常典型的算法就是那个时候发展出来的（**快速排序**）

![](/img/in-post/cs224n/fig27.jpg)

### 统计方式

![](/img/in-post/cs224n/fig28.jpg)

使用贝叶斯方式，但是需要很多平行语料，词对词的一种映射方式

![](/img/in-post/cs224n/fig29.jpg)

但是，这种方式特别复杂，主要在于映射方式多种多样。

#### 使用启发式搜索算法

但是，非常复杂以及非常需要人工的参与

## NMT
![](/img/in-post/cs224n/fig30.jpg)

一种神经网络模型的机器翻译模型，使用beam技巧来寻找最优的路径

![](/img/in-post/cs224n/fig31.jpg)


### 注意力机制

> NMT到底有哪些问题呢？需要遍历整个文章？不需要的！！！

![](/img/in-post/cs224n/fig32.jpg)

![](/img/in-post/cs224n/fig33.jpg)

### Seq2Seq应用

![](/img/in-post/cs224n/fig34.jpg)




