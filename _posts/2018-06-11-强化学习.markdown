---
layout:     post
title:      "强化学习"
subtitle:   "强化学习"
date:       2018-07-28
author:     "hadxu"
header-img: "img/hadxu.jpg"
tags:
    - Python
    - 强化学习
---

# 强化学习

未来的论文方向就应该是强化学习方向吧，使用深度学习方面的信息应用于强化学习是非常酷的。

强化学习早就有了，早在19世纪就有强化学习这个概念，尤其是Q-learning。通过更改Q值来实现选择最大的奖赏对应的动作。具体的Q-learning我有知乎专栏[q-learning](https://zhuanlan.zhihu.com/p/29213893)

## 强化学习基本概念

首先考虑基本强化学习，对于一个状态s，有各种各样的操作a，我们假设s和a都是离散的(后面会考虑连续，其实状态每一帧就是离散的)，那么在一个动作a下，就会发生状态转移，就会有下一个状态，一般来说，这种状态转移是满足马尔科夫性质的，也就是将来的状态只与现在有关，而与过去是没有关系的。

## model-free 与 model-based

model-based可以看做是在现实世界了玩，而model-free是在虚拟的世界里玩，而现在主流使用model-free，成本低。

## on-policy与off-policy

这两者的区别在于一个是一个是边玩边学，另一种是玩完了再学。

## Q-learning

在强化学习历史中，最有代表性的就是q值学习。

典型的q学习是

![](/img/in-post/rl/5-1.png)

```Python
q_values      = model(state)
next_q_values = model(next_state)

q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)
next_q_value     = next_q_values.max(1)[0]
expected_q_value = reward + gamma * next_q_value * (1 - done)

loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()
```

## policy_gradient

上述的方法是间接的实现动作的选取，而policy_gradient是直接的端到端的学习，输入是像素图像，输出的是各个动作的概率。

```Python

```

## A2C

将Q值学习与策略学习结合起来实现采取动作

```Python
class ActorCritic(nn.Module):
    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):
        super(ActorCritic, self).__init__()
        
        self.critic = nn.Sequential(
            nn.Linear(num_inputs, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
        
        self.actor = nn.Sequential(
            nn.Linear(num_inputs, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_outputs),
            nn.Softmax(dim=1),
        )
        
    def forward(self, x):
        value = self.critic(x)
        probs = self.actor(x)
        dist  = Categorical(probs)
        return dist, value
```

而根据论文的推荐

```Python
advantage = returns - values

actor_loss  = -(log_probs * advantage.detach()).mean()
critic_loss = advantage.pow(2).mean()

loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy
```

## A3C

而A3C则是A2C的基础上进行异步训练，使得训练时间大大减少。











