---
layout:     post
title:      "TitanicMachine Learning from Disaster"
subtitle:   "kaggle"
date:       2018-01-30
author:     "hadxu"
header-img: "img/in-post/cs224n/cs224n_head.png"
tags:
    - Kaggle
    - Python
---

# TitanicMachine Learning from Disaster
***
ä¸çŸ¥é“ä¸ºå•¥ï¼Œçªç„¶å–œæ¬¢æ‰“æ¯”èµ›äº†ï¼Œç„¶è€Œè‡ªå·±çš„å®é™…æ“ä½œèƒ½åŠ›å¤ªå¼±äº†ï¼Œè¿™å‡ å¤©æ•´ç†äº†ä¸€ä¸‹```TitanicMachine Learning from Disaster```çš„æ¯”èµ›ï¼Œä»å¼€å§‹å»ºæ¨¡ï¼Œåˆ°æœ€åæäº¤æˆç»©ï¼Œå°†æ•´ä¸ªç®—æ³•çš„æµç¨‹æ¢³ç†äº†ä¸€éï¼Œå¸Œæœ›å¯¹è¯»è€…æœ‰ç”¨ã€‚

## baseline
* é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¯¥æ•°æ®é›†

```python
# Import modules
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.metrics import accuracy_score

# Figures inline and set visualization style
%matplotlib inline
sns.set()
```

* å¯¼å…¥æ•°æ®é›†

```python
# Import test and train datasets
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')

# View first lines of training data
df_train.head(n=4)
```

* ç»“æœä¸º

![](/img/in-post/2018-01-3001.jpg)

* æŸ¥çœ‹dfä¿¡æ¯

```python
df_train.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
memory usage: 83.6+ KB
```

> å¯ä»¥çœ‹å‡ºæ¥ï¼Œæœ‰äº›åˆ—æ˜¯```Nan```çš„ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ç”Ÿå­˜è€…è¿›è¡Œåˆ†æ

```python
sns.countplot(x='Survived',data=df_train)
```

![](/img/in-post/Jietu20180130-212117.jpg)

> å¯ä»¥çœ‹åˆ°æ´»ç€çš„äººå¾ˆå°‘ï¼Œäºæ˜¯æˆ‘ä»¬å…ˆæäº¤å…¨æ˜¯ä¸º0çš„ç»“æœã€‚

```python
df_test['Survived'] = 0
df_test[['PassengerId','Survived']].to_csv('no_survivors.csv',index=False)
```

>å‡†ç¡®ç‡åœ¨60%å·¦å³ï¼Œä¸æ˜¯ç‰¹åˆ«å·®ï¼ŒğŸ˜€

## ç¬¬ä¸€æ¬¡æ”¹è¿›

> æˆ‘ä»¬ç›´è§‰å°±æ˜¯ï¼Œåœ¨ç”·å¥³ä¸­ï¼Œç”Ÿå­˜çš„æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ

```python
sns.countplot(x='Sex',data=df_train)
sns.factorplot(x='Survived',col='Sex',kind='count',data=df_train)
```
![](/img/in-post/Jietu20180130-212706.jpg)


* è¿˜å¯ä»¥æ¢ç´¢å„ç§å„æ ·çš„ç‰¹å¾ã€‚

## ç¬¬äºŒæ¬¡æ”¹è¿›

* å°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ”¾åœ¨ä¸€èµ·

```python
# Store target variable of training data in a safe place
survived_train = df_train.Survived

# Concatenate training and test sets
data = pd.concat([df_train.drop(['Survived'], axis=1), df_test])

data['Age'] = data.Age.fillna(data.Age.median())
data['Fare'] = data.Fare.fillna(data.Fare.median())

# Check out info of data
data.info()
```

* å°†å˜é‡ç¦»æ•£åŒ–**éå¸¸é‡è¦**

```python
data = pd.get_dummies(data, columns=['Sex'], drop_first=True)
```

* æå–ç‰¹å¾

```python
data = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]
```

* è®­ç»ƒé›†æµ‹è¯•é›†åˆ†å¼€

```python
data_train = data.iloc[:891]
data_test = data.iloc[891:]
```

```python
X = data_train.values
test = data_test.values
y = survived_train.values
```

* è®­ç»ƒ

```
clf = tree.DecisionTreeClassifier(max_depth=3)
clf.fit(X, y)
```

* é¢„æµ‹

```python
Y_pred = clf.predict(test)
df_test['Survived'] = Y_pred
```

* åœ¨è¿™é‡Œæœ‰ä¸€ä¸ªç–‘é—®ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆé€‰å–æ ‘çš„æ·±åº¦ä¸º3ï¼Ÿ

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42, stratify=y)
dep = np.arange(1, 9)
train_accuracy = np.empty(len(dep))
test_accuracy = np.empty(len(dep))

# Loop over different values of k
for i, k in enumerate(dep):
    # Setup a k-NN Classifier with k neighbors: knn
    clf = tree.DecisionTreeClassifier(max_depth=k)

    # Fit the classifier to the training data
    clf.fit(X_train, y_train)

    #Compute accuracy on the training set
    train_accuracy[i] = clf.score(X_train, y_train)

    #Compute accuracy on the testing set
    test_accuracy[i] = clf.score(X_test, y_test)

# Generate plot
plt.title('clf: Varying depth of tree')
plt.plot(dep, test_accuracy, label = 'Testing Accuracy')
plt.plot(dep, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Depth of tree')
plt.ylabel('Accuracy')
plt.show()
```

ç»“æœå¦‚

![](/img/in-post/Jietu20180130-215725.jpg)



## ç¬¬ä¸‰æ¬¡æ”¹è¿›(æ›´å¤šç‰¹å¾)

* å°†å§“åæå–å‡ºæ¥

```python
data['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))
sns.countplot(x='Title', data=data);
plt.xticks(rotation=45);
```

![](/img/in-post/Jietu20180130-214113.jpg)

* å°†èº«ä»½æå‡ºå‡ºæ¥

```python
data['Title'] = data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})
data['Title'] = data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',
                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')
sns.countplot(x='Title', data=data);
plt.xticks(rotation=45);
```

![](/img/in-post/Jietu20180130-214222.jpg)

* æ˜¯å¦æœ‰cabin

```python
data['Has_Cabin'] = ~data.Cabin.isnull()
```

* åˆ é™¤æ— ç”¨ç‰¹å¾

```python
data.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis=1, inplace=True)
data['Age'] = data.Age.fillna(data.Age.median())
data['Fare'] = data.Fare.fillna(data.Fare.median())
data['Embarked'] = data['Embarked'].fillna('S')
data.info()
```

* å°†å¹´é¾„ã€Fareè¿›è¡Œåˆ†åŒºé—´

```python
data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )
data['CatFare']= pd.qcut(data.Fare, q=4, labels=False)
data = data.drop(['Age', 'Fare'], axis=1)
```

* æ„é€ ç‰¹å¾

```python
data['Fam_Size'] = data.Parch + data.SibSp
data = data.drop(['SibSp','Parch'], axis=1)
```

* è¿›è¡Œé‡åŒ–

```python
data_dum = pd.get_dummies(data, drop_first=True)
```

* æ„é€ æ•°æ®é›†

```
# Split into test.train
data_train = data_dum.iloc[:891]
data_test = data_dum.iloc[891:]

# Transform into arrays for scikit-learn
X = data_train.values
test = data_test.values
y = survived_train.values
```

* è®­ç»ƒ

```python
dep = np.arange(1,9)
param_grid = {'max_depth' : dep}

# Instantiate a decision tree classifier: clf
clf = tree.DecisionTreeClassifier()

# Instantiate the GridSearchCV object: clf_cv
clf_cv = GridSearchCV(clf, param_grid=param_grid, cv=5)

# Fit it to the data
clf_cv.fit(X, y)

# Print the tuned parameter and score
print("Tuned Decision Tree Parameters: {}".format(clf_cv.best_params_))
print("Best score is {}".format(clf_cv.best_score_))
```

* ä¿å­˜

```
Y_pred = clf_cv.predict(test)
df_test['Survived'] = Y_pred
df_test[['PassengerId', 'Survived']].to_csv('dec_tree_feat_eng.csv', index=False)
```


**æœ€ç»ˆçš„æ•ˆæœå¤§æ¦‚æ˜¯78.468ï¼Œæ•ˆæœè¿˜ä¸é”™ï¼Œæ¥ä¸‹æ¥è€ƒè™‘ä½¿ç”¨æ¨¡å‹èåˆã€‚**





