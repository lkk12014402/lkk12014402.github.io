---
layout:     post
title:      "DAG无向图中文分词算法"
subtitle:   "中文分词"
date:       2018-01-19
author:     "hadxu"
header-img: "img/in-post/cs224n/cs224n_head.png"
tags:
    - 中文分词
    - Python
---

## 2018-01-20更新HMM算法解决OOV问题
> OOV(Out of Vocabulary)是常见的分词的难点，对于大规模真实文本来说，未登录词对于分词的精度的影响远超歧义切分。一些网络新词，自造词一般都属于这些词，因为这些词在字典中不存在，因此使用DAG图来解决分割问题是不太可行的，由此使用HMM来进行分词。

* HMM的三个问题

    a. 评估问题(概率计算问题) 
    即给定观测序列 O=O1,O2,O3…Ot和模型参数λ=(A,B,π)，怎样有效计算这一观测序列出现的概率. 
    (Forward-backward算法) 

    b. 解码问题(预测问题) 
    即给定观测序列 O=O1,O2,O3…Ot和模型参数λ=(A,B,π)，怎样寻找满足这种观察序列意义上最优的隐含状态序列S。 
    (viterbi算法,近似算法) 

    c. 学习问题 
    即HMM的模型参数λ=(A,B,π)未知，如何求出这3个参数以使观测序列O=O1,O2,O3…Ot的概率尽可能的大. 
    (即用极大似然估计的方法估计参数,Baum-Welch,EM算法)


* 而我们的中文分词则是一种解码问题，根据给定的观测序列，以及模型参数求最佳的隐含状态序列。
> 在我们的中文分词中，找到一个最佳的分词方法使得该分词的概率路径最大。此法适用于在词典中没有出现的词。而找到一种最佳的隐含状态序列即为维**维比特算法**。具体参考李航的《统计学习方法》书第185页的关于维比特算法的解释。

而维比特算法则是

1. 初始化
2. 递推 t=2,3,4,....T
3. 终止


在我们的分词中，HMM模型是五元组，分别为：

1. 状态空间（即一个句子里面的单词分为BMES，表示的为每个词在该词语中的位置B：开始，M：中间 E：结尾 ，S:单个词。）

2. 观察空间（就是我们的输入的单词。）

3. 开始状态分布（状态的初始分布）

4. 状态转移概率 （各个状态之间转移的概率）

5. 发射概率（在当前状态转移到其他词的概率）





> 值得注意的是，我们有状态转移矩阵，但是转移矩阵有一定的限制，即B后面只可能接(M or E)，不可能接(B or E)。而M后面也只可能接(M or E)，不可能接(B, S)。
根据这个逻辑，我们查看jieba源码中对状态转移矩阵的限制：

```
PrevStatus = {  
    'B':('E','S'),  
    'M':('M','B'),  
    'S':('S','E'),  
    'E':('B','M')  
}  
```
维比特算法：

```
def viterbi(obs,states,start_p, trans_p, emit_p):
	V = [{}]
	path = {}

	"""
	emit_p 发射概率(当前状态(BESM)到观察其他词的概率)

	"""
	for y in states:
		V[0][y] = start_p[y]+emit_p[y].get(obs[0],MIN_FLOAT)
		path[y] = [y]
	for t in range(1,len(obs)):
		V.append({})
		newpath = {}
		for y in states:
			em_p = emit_p[y].get(obs[t], MIN_FLOAT)
		# t时刻状态为y的最大概率(从t-1时刻中选择到达时刻t且状态为y的状态y0)
			(prob, state) = max([(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])
			V[t][y] = prob
			newpath[y] = path[state] + [y] # 只保存概率最大的一种路径
		path = newpath
		# 求出最后一个字哪一种状态的对应概率最大，最后一个字只可能是两种情况：E(结尾)和S(独立词)  
	(prob, state) = max((V[len(obs) - 1][y], y) for y in 'ES')

	return (prob, path[state])
```

当我们将一个句子的HMM最佳概率算出来，比如“京研”，使用DAG图切分“京/研”,而我们使用维比特算法的话，得到的结果为

```
-17.377835407692192 ['B', 'E']
```
即表示该一个词一个B，一个E，就是一个开头，一个结尾,那即是一个词，即为“京研”，当我们将该最佳切分算出来，就可以进行构造分词

```
def __cut(sentence):
	# viterbi算法得到sentence 的切分
	prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P)

	begin, nexti = 0, 0
	print(prob, pos_list)
	for i, char in enumerate(sentence):
		pos = pos_list[i]
		if pos == 'B':
			begin = i
		elif pos == 'E':
			yield sentence[begin:i + 1]
			nexti = i + 1
		elif pos == 'S':
			yield char
			nexti = i + 1
	if nexti < len(sentence):
		yield sentence[nexti:]
```
那么，关于HMM的分词算法就到此结束了。

# DAG无向图中文分词算法

## 分词背景
着几天一直在学习CS224N的NLP课程，但是感觉CS224N课程并没有分词这一个步骤，原因在于语言上的差异。英语天生的以空格来进行分词，而中文没有，只有一句话结束才有一个分隔符，因此要将CS224N中学习到的技术运用在中文上，需要进行分词这一个步骤。但是，研究人员特别少，但是网上有一个博客写的非常好，[blog](http://blog.csdn.net/daniel_ustc/article/details/48223135),剖析```jieba```中文分词库。于是本人就将该库```clone```到本地，仔细研读该代码。

### DAG无向图构造
DAG是无环图的缩写，目的是将一个句子使用图的形式来表示，同时DAG的构造需要词典，词典dict.txt形式如下：

```
A座 3 n
A股 3 n
A型 3 n
A轮 3 n
AA制 3 n
AB型 3 n
...
```
该词典的数据结构为(词语 出现次数 词性)。

比如句子”去北京大学玩“对应的DAG为:

{0 : [0], 1 : [1, 2, 4], 2 : [2], 3 : [3, 4], 4 : [4], 5 : [5]}

例如DAG中{0:[0]} 这样一个简单的DAG, 就是表示0位置对应的是词, 就是说0~0,即”去”这个词 在dict.txt中是词条。DAG中{1:[1,2,4]}, 就是表示1位置开始, 在1,2,4位置都是词, 就是说1~1,1~2,1~4 即 “北”，“北京”，“北京大学”这三个词 在dict.txt对应文件的词库中。

在jieba分词中，首先将dict.txt字典解析成二进制文件，便于后来读取。

```
def check_initialized(self):
    if not self.initialized:
        abs_path = _get_abs_path(self.dictionary)
        if self.cache_file:
            cache_file = self.cache_file
        # 默认的cachefile
        elif abs_path:
            cache_file = "jieba.cache"

        load_from_cache_fail = True
        # cachefile 存在
        if os.path.isfile(cache_file):

            try:
                with open(cache_file, 'rb') as cf:
                    self.FREQ, self.total = marshal.load(cf)
                load_from_cache_fail = False
            except Exception:
                load_from_cache_fail = True
        if load_from_cache_fail:
            self.FREQ, self.total = self.gen_pfdict(abs_path)
            # 把dict前缀集合,总词频写入文件
            try:
                with open(cache_file, 'w') as temp_cache_file:
                    marshal.dump((self.FREQ, self.total), temp_cache_file)
            except Exception:
                # continue
                pass
        # 标记初始化成功
        self.initialized = True
```

当词典构建好，就可以对我们的词进行DAG。

```
s = '今天是2015年9月3号，去天安门广场庆祝抗战胜利70周年'

DAG = {}

N = len(s)

for k in range(N):
	templist = []
	i = k
	frag = s[k]

	while i<N and frag in FREQ: # 如果词语在词典里，则将当前的下标存放
		if FREQ[frag]:
			templist.append(i)
		i += 1
		frag = s[k:i+1] #获取下一个词
	if not templist:
		templist.append(k)
	DAG[k] = templist
```

当我们将句子的DAG建立好，就可以使用动态规划来求解最大概率的词的组成：

**值得注意的是，在jieba分词中，使用了概率取对数实现的求最大概率路径问题，原因在于当词汇量过大可能出现溢出的问题。**

```
def calc(sentence,DAG,route):
	N = len(sentence)
	route[N] = (0,0)

	logtotal = log(total)

    # 列表推倒求最大概率对数路径
    # route[idx] = max([ (概率对数，词语末字位置) for x in DAG[idx] ])
    # 以idx:(概率对数最大值，词语末字位置)键值对形式保存在route中
    # route[x+1][0] 表示 词路径[x+1,N-1]的最大概率对数,
    # [x+1][0]即表示取句子x+1位置对应元组(概率对数，词语末字位置)的概率对数

	for idx in range(N-1,-1,-1):
		route[idx] = [(log(FREQ.get(sentence[idx:x+1]) or 1) - 
			logtotal + route[x+1][0],x) for x in DAG[idx]]

		route[idx] = max(route[idx])
```

当最大的分割构建好，使用无HMM的方式来将词语解析出来

```
def cut_DAG_NO_HMM(sentence):
	x = 0
	N = len(sentence)

	buf = ''

	while x<N:
		y = route[x][1]+1
		l_word = sentence[x:y]
        # 使用re_eng 来判断当前的词是否为字母或数字
        # re_eng = re.compile('[a-zA-Z0-9]', re.U)
		if re_eng.match(l_word) and len(l_word)==1:
			buf += l_word
			x = y
		else:
			if buf:
				yield buf
				buf = ''
			yield l_word
			x = y
	if buf:
		yield buf
		buf = ''
```

根据我们的算法，求得上述的分词结果为：

```
今天/是/2015/年/9/月/3/号/，/去/天安门广场/庆祝/抗战/胜利/70/周年
```

明显发现，分词的好与不好，是建立在词典的基础之上。





